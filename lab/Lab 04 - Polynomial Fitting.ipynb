{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 04 - Polynomial Fitting\n",
    "In the previous lab we discussed linear regression and the OLS estimator for solving the minimization of the RSS. As we\n",
    "mentioned, regression problems are a very wide family of settings and algorithms which we use to try to estimate the relation between a set of explanatory variables and a **continuous** response (i.e. $\\mathcal{Y}\\in\\mathbb{R^p}$). In the following lab we will discuss one such setting called \"Polynomial Fitting\". \n",
    "\n",
    "Sometimes, the data (and the relation between the explanatory variables and response) can be described by some polynomial\n",
    "of some degree. Here, we only focus on the case where it is a polynomial of a single variable. That is: \n",
    "$$ p_k\\left(x\\right)=\\sum_{i=0}^{k}\\alpha_i x_i^k\\quad\\alpha_1,\\ldots,\\alpha_k\\in\\mathbb{R} $$\n",
    "\n",
    "So our hypothesis class is of the form:\n",
    "$$ \\mathcal{H}^k_{poly}=\\left\\{p_k|p_k\\left(x\\right)=\\sum_{i=0}^{k}\\alpha_i x_i^k\\quad\\alpha_1,\\ldots,\\alpha_k\\in\\mathbb{R}\\right\\} $$\n",
    "\n",
    "Notice that similar to linear regression, each hypothesis in the class is defined by a coefficients vector. Below are two\n",
    "examples (simulated and real) for datasets where the relation between the explanatory variable and response is polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = lambda x: x**4 - 2*x**3 - .5*x**2 + 1\n",
    "\n",
    "x = np.linspace(-1.2, 2, 20)\n",
    "y_ = response(x)\n",
    "\n",
    "df = pd.read_csv(\"../data/Position_Salaries.csv\", skiprows=2, index_col=0)\n",
    "x2, y2 = df.index, df.Salary\n",
    "\n",
    "make_subplots(1, 2, subplot_titles=(r\"$\\text{Simulated Data: }y=x^4-2x^3-0.5x^2+1$\", r\"$\\text{Positions Salary}$\"))\\\n",
    "    .add_traces([go.Scatter(x=x, y=y_, mode=\"markers\", marker=dict(color=\"black\", opacity=.7), showlegend=False),\n",
    "                 go.Scatter(x=x2, y=y2, mode=\"markers\",marker=dict(color=\"black\", opacity=.7), showlegend=False)], \n",
    "                rows=[1,1], cols=[1,2])\\\n",
    "    .update_layout(title=r\"$\\text{(1) Datasets For Polynomial Fitting}$\", margin=dict(t=100)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we have discussed in class, solving a polynomial fitting problem can be done by first manipulating the input data,\n",
    "such that we represent each sample $x_i\\in\\mathbb{R}$ as a vector $\\mathbf{x}_i=\\left(x^0,x^1,\\ldots,x^k\\right)$. Then,\n",
    "we treat the data as a design matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times k}$ of a linear regression problem.\n",
    "\n",
    "For the simulated dataset above, which is of a polynomial of degree 4, the design matrix looks as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "m, k, X = 5, 4, x.reshape(-1, 1)\n",
    "pd.DataFrame(PolynomialFeatures(k).fit_transform(X[:m]), \n",
    "             columns=[rf\"$x^{{0}}$\".format(i) for i in range(0, k+1)],\n",
    "             index=[rf\"$x_{{0}}$\".format(i) for i in range(1, m+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fitting A Polynomial Of Different Degrees\n",
    "\n",
    "Next, let us fit polynomials of different degrees and different noise properties to study how it influences the learned model.\n",
    "We begin with the noise-less case where we fit for different values of $k$. As we increase $k$ we manage to fit a model\n",
    "that describes the data in a better way, reflected by the decrease in the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ks = [2, 3, 4, 5]\n",
    "fig = make_subplots(1, 4, subplot_titles=list(ks))\n",
    "for i, k in enumerate(ks):\n",
    "    y_hat = make_pipeline(PolynomialFeatures(k), LinearRegression()).fit(X, y_).predict(X)\n",
    "    \n",
    "    fig.add_traces([go.Scatter(x=x, y=y_, mode=\"markers\", name=\"Real Points\",  marker=dict(color=\"black\", opacity=.7), showlegend=False),\n",
    "                    go.Scatter(x=x, y=y_hat, mode=\"markers\", name=\"Predicted Points\",  marker=dict(color=\"blue\", opacity=.7), showlegend=False)], rows=1, cols=i+1)\n",
    "    fig[\"layout\"][\"annotations\"][i][\"text\"] = rf\"$k={{0}}, MSE={{1}}$\".format(k, round(np.mean((y_-y_hat)**2), 2))\n",
    "\n",
    "fig.update_layout(title=r\"$\\text{(2) Simulated Data - Fitting Polynomials of Different Degrees}$\",\n",
    "                  margin=dict(t=60),\n",
    "                  yaxis_title=r\"$\\widehat{y}$\",\n",
    "                  height=300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we find the right $k$ (which in our case is 4) we managed to fit a perfect model, after which, as we increase $k$, \n",
    "the additional coefficients will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coefs = {}\n",
    "for k in ks:\n",
    "    fit = make_pipeline(PolynomialFeatures(k), LinearRegression()).fit(X, y_)\n",
    "    coefs[rf\"$k={{{k}}}$\"] = [round(c,3) for c in fit.steps[1][1].coef_]\n",
    "pd.DataFrame.from_dict(coefs, orient='index', columns=[rf\"$w_{{{i}}}$\" for i in range(max(ks)+1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fitting Polynomial Of Different Degrees - With Sample Noise\n",
    "\n",
    "Still fitting for different values of $k$, let us add some standard Gaussian noise (i.e. $\\mathcal{N}\\left(0,1\\right)$).\n",
    "This time we observe two things:\n",
    "- Even for the correct $k=4$ model we are not able to achieve zero MSE.\n",
    "- As we increase $4<k\\rightarrow 7$ we manage to decrease the error more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = y_ + np.random.normal(size=len(y_))\n",
    "\n",
    "ks = range(2, 8)\n",
    "fig = make_subplots(2, 3, subplot_titles=list(ks))\n",
    "for i, k in enumerate(ks):\n",
    "    r,c = i//3+1, i%3+1 \n",
    "    \n",
    "    y_hat = make_pipeline(PolynomialFeatures(k), LinearRegression()).fit(X, y).predict(X)\n",
    "    fig.add_traces([go.Scatter(x=x, y=y_, mode=\"markers\", name=\"Real Points\",  marker=dict(color=\"black\", opacity=.7), showlegend=False),\n",
    "                    go.Scatter(x=x, y=y, mode=\"markers\", name=\"Observed Points\",  marker=dict(color=\"red\", opacity=.7), showlegend=False), \n",
    "                    go.Scatter(x=x, y=y_hat, mode=\"markers\", name=\"Predicted Points\",  marker=dict(color=\"blue\", opacity=.7), showlegend=False)], rows=r, cols=c)\n",
    "    fig[\"layout\"][\"annotations\"][i][\"text\"] = rf\"$k={{0}}, MSE={{1}}$\".format(k, round(np.mean((y-y_hat)**2), 2))\n",
    "\n",
    "fig.update_layout(title=r\"$\\text{(4) Simulated Data With Noise - Fitting Polynomials of Different Degrees}$\", margin=dict(t=80)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How is it that we are able to fit \"better\" models for $k$s larger than the true one? As we increase $k$ we enable the model\n",
    "more \"degrees of freedom\" to try and adapt itself to the observed data. The higher $k$ the more the learner will \"go after\n",
    "the noise\" and miss the real signal of the data. In other words, what we have just observed is what is known as **overfitting**.\n",
    "\n",
    "Later in the course we will learn methods for detection and avoidance of overfitting.\n",
    "\n",
    "\n",
    "## Fitting Polynomial Over Different Sample Noise Levels\n",
    "\n",
    "Next, let us set $k=4$ (the true values) and study the outputted models when training over different noise levels. Though\n",
    "we will only be changing the scale of the noise (i.e. the variance, $\\sigma^2$), changing other properties such as its\n",
    "distribution is interesting too. As we would expect, as we increase the scale of the noise our error increases. We can\n",
    "observe this also in a visual manner, where the fitted polynomial (in blue) less and less resembles the actual model (in black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scales = range(6)\n",
    "fig = make_subplots(2, 3, subplot_titles=list(map(str, scales)))\n",
    "for i, s in enumerate(scales):\n",
    "    r,c = i//3+1, i%3+1\n",
    "    \n",
    "    y = y_ + np.random.normal(scale=s, size=len(y_))\n",
    "    y_hat = make_pipeline(PolynomialFeatures(4), LinearRegression()).fit(X, y).predict(X)\n",
    "\n",
    "    fig.add_traces([go.Scatter(x=x, y=y_, mode=\"markers\", name=\"Real Points\",  marker=dict(color=\"black\", opacity=.7), showlegend=False),\n",
    "                    go.Scatter(x=x, y=y, mode=\"markers\", name=\"Observed Points\",  marker=dict(color=\"red\", opacity=.7), showlegend=False),\n",
    "                    go.Scatter(x=x, y=y_hat, mode=\"markers\", name=\"Predicted Points\",  marker=dict(color=\"blue\", opacity=.7), showlegend=False)], rows=r, cols=c)\n",
    "    fig[\"layout\"][\"annotations\"][i][\"text\"] = rf\"$\\sigma^2={{0}}, MSE={{1}}$\".format(s, round(np.mean((y-y_hat)**2), 2))\n",
    "\n",
    "fig.update_layout(title=r\"$\\text{(5) Simulated Data - Different Noise Scales}$\", margin=dict(t=80)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Influence Of $k$ And $\\sigma^2$ On Error\n",
    "\n",
    "Lastly, let us check how the error is influenced by both $k$ and $\\sigma^2$. For each value of $k$ and $\\sigma^2$ we will\n",
    "add noise drawn from $\\mathcal{N}\\left(0,\\sigma^2\\right)$ and then, based on the noisy data, let the learner select an\n",
    "hypothesis from $\\mathcal{H}_{poly}^k$. We repeat the process for each set of $\\left(k,\\sigma^2\\right)$ 10 times and report\n",
    "the mean MSE value. Results are seen in heatmap below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "df = []\n",
    "for setting in ParameterGrid(dict(k=range(10), s=np.linspace(0, 5, 10), repetition=range(10))):\n",
    "    y = y_ + np.random.normal(scale=setting[\"s\"], size=len(y_))\n",
    "    y_hat = make_pipeline(PolynomialFeatures(setting[\"k\"]), LinearRegression()).fit(X, y).predict(X)\n",
    "    \n",
    "    df.append([setting[\"k\"], setting[\"s\"], np.mean((y-y_hat)**2)])\n",
    "    \n",
    "df = pd.DataFrame.from_records(df, columns=[\"k\", \"sigma\",\"mse\"]).groupby([\"k\",\"sigma\"]).mean().reset_index()\n",
    "\n",
    "go.Figure(go.Heatmap(x=df.k, y=df.sigma, z=df.mse, colorscale=\"amp\"),\n",
    "          layout=go.Layout(title=r\"$\\text{(6) Average Train } MSE \\text{ As Function of } \\left(k,\\sigma^2\\right)$\", \n",
    "                           xaxis_title=r\"$k$ - Fitted Polynomial Degree\",\n",
    "                           yaxis_title=r\"$\\sigma^2$ - Noise Levels\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Time To Think...\n",
    "\n",
    "In the above figure, we observe the following trends:\n",
    "- As already seen before, for the noise-free data, once we reach the correct $k$ we achieve zero MSE.\n",
    "- Across all values of $k$, as we increase $\\sigma^2$ we get higher MSE values.\n",
    "- For all noise levels, we manage to reduce MSE values by increasing $k$.\n",
    "\n",
    "So, by choosing a **richer** hypothesis class (i.e. larger and that can express more functions - polynomials of higher\n",
    "degree) we are able to choose an hypothesis that fits the **observed** data **better**, regardless to how noisy the data is.\n",
    "Try and think how the above heatmap would look if instead of calculating the MSE over the training samples (i.e train error)\n",
    "we would have calculated it over a **new** set of test samples drawn from the same distribution.\n",
    "\n",
    "Use the below code to create a test set. Change the code generating figure 6 such that the reported error is a test error. Do not forget to add the noise (that depends on $\\sigma^2$) to the test data. What has changed between what we observe for the train error to the test error? What happens for high/low values of $\\sigma^2$? What happens for high/low values of $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.linspace(-1.2, 2, 40)[1::2].reshape(-1,1)\n",
    "testY = response(testX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
