{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 03 - Simple Linear Regression\n",
    "\n",
    "Regressions are any learning problem that aim to describe the relation between a set of explanatory \n",
    "variables (i.e. features) and a continuous response (or a set of responses). Therefore our dataset is of the form:\n",
    "\n",
    "$$S=\\left\\{\\left(\\mathbf{x}_i, y_i\\right)\\right\\}^m_{i=1} \\quad s.t. \\quad \\mathbf{x}_i\\in\\mathbb{R^d},\\,\\,y_i\\in\\mathbb{R}$$\n",
    "\n",
    "In the case of Linear Regression the relation learned is a linear one. That is, we search for a linear function to map \n",
    "$\\mathcal{X}$ to $\\mathcal{Y}$. So the hypothesis class of linear regression is:\n",
    "\n",
    "$$ \\mathcal{H}_{reg} = \\left\\{h:h\\left(x_1,\\ldots,x_d\\right)=w_0 + \\sum w_i x_i\\right\\} $$\n",
    "\n",
    "Note that the linear function is linear in the parameters $w_0,w_1,\\ldots,w_d$. Let us simulate a dataset fitting the case of a simple linear regression: \n",
    "\n",
    "$$ y_i = w_1 x_i + w_0 \\quad i=1,\\ldots,m $$\n",
    "\n",
    "So each hypothesis in the class $\\mathcal{H}_{reg}$ is defined by two parameters $w_0,w_1$ - the intercept and slope of\n",
    "the line. Suppose the data is generated from the following line: $Y=2X+1$. So $w_0=1$ and $w_2=2$. Let us draw and plot \n",
    "samples from this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w0, w1 = 1, 2    \n",
    "\n",
    "x = np.linspace(0, 100, 10)\n",
    "y = w1*x + w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter(x=x, y=y, name=\"Real Model\", showlegend=True,\n",
    "                                 marker=dict(color=\"black\", opacity=.7), line=dict(color=\"black\", dash=\"dash\", width=1))], \n",
    "          layout=go.Layout(title=r\"$\\text{(1) Simulated Data}$\",\n",
    "                           xaxis={\"title\": \"x - Explanatory Variable\"},\n",
    "                           yaxis={\"title\": \"y - Response\"},\n",
    "                           height=400))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using this sample as a **training set**, let us compute the Ordinary Least Squares (OLS) estimators $\\widehat{w_0},\\widehat{w_1}$ of the model. Then, if we are given new samples $x_j$ we can predict its response $\\widehat{y}_j$:\n",
    "\n",
    "$$ \\widehat{y}_j = \\widehat{w_1} x_j + \\widehat{w}_0 $$\n",
    "\n",
    "Over the dataset above, try and think what would you expect the output to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "noiseless_model = LinearRegression()\n",
    "\n",
    "noiseless_model.fit(x.reshape((-1,1)), y)\n",
    "print(\"Estimated intercept:\", noiseless_model.intercept_)\n",
    "print(\"Estimated coefficient:\", noiseless_model.coef_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression With Noise\n",
    "As the dataset used to fit the model lays exactly on a straight line, the estimated coefficients are the correct \n",
    "ones (up to floating point precision). Next, let us add some Gaussian noise to the data and see how it influences our \n",
    "estimation. So: \n",
    "\n",
    "$$\\forall i \\in \\left[ m \\right]\\quad y_i=w_1\\cdot x_i + w_0 + \\varepsilon_i \\quad s.t.\\quad \n",
    "\\varepsilon\\sim\\mathcal{N}\\left(0,\\sigma^2I_m\\right)$$\n",
    "\n",
    "Namely, the noise of each sample distributes by a Gaussian with zero mean and $\\sigma^2$ variance, and is uncorrelated between samples.\n",
    "\n",
    "*Notice that from now on we mark the $y$'s generated by the noise-less model with `y_`. This is so it is clear that the \"real\"\n",
    "$y$'s observed in a given sample are noisy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"y_\" not in locals(): y_ = y\n",
    "epsilon = np.random.normal(loc=0, scale=40, size=len(x))\n",
    "y = y_ + epsilon\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=y, name=\"Observed Points\", mode=\"markers\", line=dict(width=1)))\n",
    "fig.update_layout(title=r\"$\\text{(2) Simulated Data - With Noise}$\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try and execute the block above several times. See how each time the \"Observed Points\" look different. These datasets,\n",
    "though all come from the same model, look very different. Try to think:\n",
    "\n",
    "* What would happen if we attempt fitting a model to these observations (i.e. the ones with the noise)? \n",
    "* How would it influence our estimation of the coefficients $w_0, w_1$? \n",
    "* Where will the regression line be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame \n",
    "model = LinearRegression().fit(x.reshape((-1,1)), y)\n",
    "\n",
    "DataFrame({\"Model\":[\"Noise-less\",\"Noisy\"], \n",
    "           \"Intercept\": [noiseless_model.intercept_, model.intercept_],\n",
    "           \"Slope\": [noiseless_model.coef_[0], model.coef_[0]]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(x.reshape(-1,1))\n",
    "\n",
    "fig.data = [fig.data[0], fig.data[1]]\n",
    "fig.update_layout(title=r\"$\\text{(3) Fitted Model Over Noisy Data}$\")\n",
    "fig.add_traces([go.Scatter(x=x, y=y_hat, mode=\"markers\", name=\"Predicted Responses\", marker=dict(color=\"blue\")),\n",
    "                go.Scatter(x=x, y=y_hat, mode=\"lines\", name=\"Fitted Model\", line=dict(color=\"blue\", width=1))])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us better understand what took place. Schematically, we started with some model\n",
    "$$ Y=w_1X+w_0 \\quad s.t. w_1=2,w_0=1 $$\n",
    "\n",
    "and obtained a dataset from this model \n",
    "$$ Y=w_1X + w_0 + \\mathcal{N}\\left(0,\\sigma^2\\right) $$ \n",
    "\n",
    "Then, using the dataset we estimated the model parameters to obtain $\\widehat{w_1},\\widehat{w_0}$. However, we should look\n",
    "at these steps from two different points of view: the \"observer\" and the \"all-knowing\".\n",
    "- The \"observer\" is us whenever we work with data. We somehow obtained samples/observations that we assume to be generated\n",
    "from some \"true\" function/model $f$. As in reality data is noisy, when we assume something about the \"true\" function we \n",
    "also make assumptions about the noise. Then, as we do not know $f$ we try to learn it based on the observations.\n",
    "- The \"all-knowing\", unlike the \"observer\", knows exactly how $f$ looks and for each sample what is the noise.  \n",
    "\n",
    "In the graph above the <span style=\"color:Black\">**Real Model**</span> is only known to the \"all-knowing\". We, as the \n",
    "\"observer\" only witness the <span style=\"color:red\">**Observed Points**</span>. We **assumed** the data came from a linear\n",
    "model with Gaussian Noise and therefore fitted the OLS estimators $\\widehat{w}_1, \\widehat{w}_0$. These estimators give\n",
    "us the <span style=\"color:blue\">**Fitted Model**</span> and a <span style=\"color:blue\">**Predicted Response**</span> to \n",
    "each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using these estimators of the model coefficients we can do two things:\n",
    "- **Inference**: We can study the estimated model. What are the statistical properties of our estimators? How confident are\n",
    "we in the estimation? Are the features associated with the helpful/relevant for predicting/explaining the response? Etc.\n",
    "- **Prediction**: We can use this estimated model to predict the responses of new data-points. How accurate are our predictions? How does the training set (and its size) influence this accuracy? \n",
    "\n",
    "In the scope of this course we are mainly interested in using the fitted model for prediction, with only slightly \n",
    "investigating the properties of our fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multivatiate Linaer Regression\n",
    "Lastly, using a more complicated model, we fit a model and answer some inference and prediction questions. \n",
    "To gain a better understanding, please look at the graph below and answer the question before reading the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = lambda x1, x2: 5*x1 + .1*x2 + 3\n",
    "\n",
    "min_x1, min_x2, max_x1, max_x2 = -10, -10, 10, 10\n",
    "xv1, xv2 = np.meshgrid(np.linspace(min_x1, max_x1, 10), np.linspace(min_x2, max_x2, 10))\n",
    "surface = response(xv1, xv2)\n",
    "\n",
    "x = np.random.uniform((min_x1, min_x2), (max_x1, max_x2), (10, 2))\n",
    "y_ = response(x[:,0], x[:,1])\n",
    "y = y_ + np.random.normal(0, 30, len(x))\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "y_hat = model.predict(x)\n",
    "\n",
    "DataFrame({\"Coefficient\": [rf\"$w_{{0}}$\".format(i) for i in range(len(model.coef_)+1)],\n",
    "           \"Estimated Value\": np.concatenate([[model.intercept_], model.coef_])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "go.Figure([go.Surface(x=xv1, y=xv2, z=surface, opacity=.5, showscale=False),\n",
    "           go.Scatter3d(name=\"Real (noise-less) Points\", x=x[:,0], y=x[:,1], z=y_,    mode=\"markers\", marker=dict(color=\"black\", size=2)),\n",
    "           go.Scatter3d(name=\"Observed Points\",          x=x[:,0], y=x[:,1], z=y,     mode=\"markers\", marker=dict(color=\"red\", size=2)),\n",
    "           go.Scatter3d(name=\"Predicted Points\",         x=x[:,0], y=x[:,1], z=y_hat, mode=\"markers\", marker=dict(color=\"blue\", size=2))],\n",
    "          layout=go.Layout(\n",
    "              title=r\"$\\text{(4) Bivariate Linear Regression}$\",\n",
    "              scene=dict(xaxis=dict(title=\"Feature 1\"),\n",
    "                         yaxis=dict(title=\"Feature 2\"),\n",
    "                         zaxis=dict(title=\"Response\"),\n",
    "                         camera=dict(eye=dict(x=-1, y=-2, z=.5)))\n",
    "          )).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Time To Think...\n",
    "In the scenario above we performed a linear regression over observations with more than two features (i.e multi-variate\n",
    "linear regression). In gradient color we see the subspace from which our data-points are drawn. As we have 2 features, the subspace is a 2D plane.\n",
    "\n",
    "Try rotating the figure above and look at the plane from its different axes (such that it looks like a line rather than a plane). This view allows you to see the fit between the one specific feature and the response, similar to the case of fitting a simple linear regression using that feature. \n",
    "\n",
    "Run the code generating the data and graph with more/less samples and high/lower noise levels. How do these changes influence the quality of the fit? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
