\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{part:intro}{{}{9}{Introduction}{part*.2}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:intro}}{9}{part*.2}}
\newlabel{part:lecture_1}{{}{10}{Lecture 1: Mathematical Background}{part*.3}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_1}}{10}{part*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Recap: Probability and Statistics}{10}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Probability Space}{10}{subsection.1.1}}
\newlabel{lem:unionBound}{{1}{11}{Probability Space}{lemma.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Random Variables, discrete and continuous}{11}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Mean and Variance}{12}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}A little Statistics: Mean and variance estimation}{14}{subsection.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}High dimensional distributions}{15}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Basic definitions}{15}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Normal Distribution}{15}{subsection.2.2}}
\newlabel{def: UVN}{{12}{15}{Univariate Normal Distribution (namely, in dimension $d=1$)}{definition.12}{}}
\newlabel{def: MVN}{{13}{16}{Multivariate Normal Distribution}{definition.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $f(x) = \frac  {1}{\sqrt  {2\pi \sigma ^2} } e^{ -\frac  {(x-\mu )^2}{2\sigma ^2} } $\relax }}{16}{figure.caption.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \begingroup \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 2D case of  $f\left (x\right )=\frac  {1}{\sqrt  {\left (2\pi \right )^{2}\left |\Sigma \right |}}\qopname  \relax o{exp}\left \{ -\frac  {\left (x-\mu \right )^{T}\Sigma ^{-1}\left (x-\mu \right )}{2}\right \} $ \endgroup \relax }}{16}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Covariance Matrix}{17}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Estimating the Covariance Matrix}{18}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Linear Transformations of the Data Set}{19}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Uncorrelated Variables\relax }}{19}{figure.caption.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Uncorrelated Scaled Variables\relax }}{20}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Correlated Scaled Variables\relax }}{21}{figure.caption.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Probability Inequalities}{21}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation and Background}{21}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Recap}{22}{subsection.3.2}}
\newlabel{sec:recap}{{3.2}{22}{Recap}{subsection.3.2}{}}
\newlabel{cor:markovAve}{{1}{22}{Recap}{corollary.1}{}}
\newlabel{eq:varAve}{{1}{23}{Recap}{equation.3.1}{}}
\newlabel{cor:chebysevAve}{{2}{23}{Recap}{corollary.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Coin Prediction}{23}{subsection.3.3}}
\newlabel{sec:basicConcentration}{{3.3}{23}{Coin Prediction}{subsection.3.3}{}}
\newlabel{def:sampleCoin}{{16}{24}{Coin Prediction}{definition.16}{}}
\newlabel{cor:basicConcentration}{{3}{25}{Coin Prediction}{corollary.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Hoeffding's Inequality}{26}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Coin Prediction Revisited}{26}{subsubsection.3.3.2}}
\newlabel{cor:basicConcentration}{{5}{26}{Coin Prediction Revisited}{corollary.5}{}}
\newlabel{part:lecture_2}{{3.3.2}{27}{Lecture 2: The Linear Model}{part*.8}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_2}}{27}{part*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}The Linear Model, Noiseless Case}{27}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Problem: Customer Lifetime Value prediction}{27}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The training data matrix}{27}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Setup}{27}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}The Linear Hypothesis Class}{28}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}The Realizable Case}{28}{subsubsection.4.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}The Non-Realizable Case}{28}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Geometric interpretation}{28}{subsubsection.4.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In realistic cases, $y$ is never exactly linear in the samples $\mathbf  {x}$\relax }}{29}{figure.caption.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Designing the Learning Algorithm}{29}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}The Loss Function}{29}{subsubsection.4.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Empirical Risk Minimization}{29}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Least Squares}{30}{subsubsection.4.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces $RSS(\mathbf  {w})=\left \delimiter "067D80F  \mathbf  {y}-X^\ensuremath  {\top }\mathbf  {w}\right \delimiter "067D80F ^2$ in the case of $X$ with a full rank - a unique minimum\relax }}{30}{figure.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}The Normal Equations}{31}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Solving the Normal Equations}{31}{subsubsection.4.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Singular Value Decomposition}{32}{subsection.4.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.1}Making the SVD solution numerically stable}{33}{subsubsection.4.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}How many samples do we need to learn a linear function?}{33}{subsection.4.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Summary - Noiseless Case}{33}{subsection.4.9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}The Linear Model - Noisy Case }{34}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Generation Model With Noise}{34}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces $\mathbf  {y}$ no longer in $Im(X^\ensuremath  {\top })$\relax }}{35}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces $\mathaccentV {hat}05E{y}\equiv X^\ensuremath  {\top }\mathaccentV {hat}05E{w}$ is the projection of $\mathbf  {y}$ onto $Im(X^\ensuremath  {\top })$\relax }}{35}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The Maximum Likelihood Principle}{35}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Noise, Bias and Variance}{36}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Polynomial fitting}{36}{subsubsection.5.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Polynomial fitting: no noise}{36}{subsubsection.5.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Bias}{39}{subsubsection.5.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Polynomial fitting: with noise}{40}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Variance}{42}{subsection.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Variance in linear model: Geometrical Interpretation}{43}{subsection.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Bias-Variance Tradeoff}{45}{subsection.5.7}}
\newlabel{part:lecture_3}{{5.7}{46}{Lecture 3: Classification}{part*.51}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_3}}{46}{part*.51}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Introduction}{46}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Textbook references}{47}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Some preliminaries}{48}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Examples for classification problems}{49}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Heart disease data}{51}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces South African Heart Data from ESL\relax }}{51}{figure.caption.53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Plotting and imagining a feature space $\ensuremath  {\mathbb  {R}}^d$ with binary labeled data}{51}{subsection.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Classification training sample in $\mathbb  {R}^2$\relax }}{52}{figure.caption.54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}What loss function should we use?}{52}{subsection.6.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Type-I and Type-II errors}{52}{subsection.6.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}False Positive, False Negative and all that jazz}{54}{subsection.6.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}Decision boundary}{55}{subsection.6.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Decision boundary of a linear classifier (source: ESL)\relax }}{55}{figure.caption.56}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{b1}{{12}{55}{Decision boundary of a linear classifier (source: ESL)\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10}Our goal in this lecture}{55}{subsection.6.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Decision boundary of a $k$-nearest-neighbor classifier (source: ESL\relax }}{56}{figure.caption.57}}
\newlabel{b2}{{13}{56}{Decision boundary of a $k$-nearest-neighbor classifier (source: ESL\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}The Half-Space classifier}{56}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Half-space defined by a vector. Source: UML\relax }}{57}{figure.caption.58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Assumption - training sample is linearly separable}{57}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Learning using ERM}{58}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Computationally implementing ERM for halfspace classifiers}{58}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Commercial break: Convex optimization}{59}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Solving ERM for half-space by linear programming}{60}{subsection.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}What about a training sample that's not linearly separable?}{60}{subsection.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Summary}{60}{subsection.7.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Support Vector Machines}{60}{section.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Which separating hyperplane should we choose? Source: UML\relax }}{61}{figure.caption.59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}A new learning principle: Maximum Margin}{61}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Hard SVM}{62}{subsection.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Soft SVM}{63}{subsection.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}A family of learners}{64}{subsection.8.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}When is SVM useful?}{64}{subsection.8.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Summary - Soft SVM}{64}{subsection.8.6}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Logistic Regression}{65}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}A probabilistic model for noisy labels}{65}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}The hypothesis class}{65}{subsection.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}The learning principle: maximum likelihood}{66}{subsection.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Computational implementation}{67}{subsection.9.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Interpretability}{68}{subsection.9.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}Which features were important}{68}{subsubsection.9.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Why was this label predicted?}{68}{subsubsection.9.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.3}Example: Interpretability of linear regression}{68}{subsubsection.9.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.4}Interpretability of logistic regression}{68}{subsubsection.9.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}How to make predictions on a new sample: working with estimated class probabilities and choosing the cutoff}{68}{subsection.9.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces ROC curves of three trained models (source:jxieeducation.com)\relax }}{70}{figure.caption.61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Summary}{71}{subsection.9.7}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Nearest Neighbors}{72}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}No hypothesis class}{72}{subsection.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Prediction with $k$-nearest neighbors}{72}{subsection.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Choosing $k$}{73}{subsection.10.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Decision boundary for a $1$-NN classifier (source: ESL)\relax }}{73}{figure.caption.62}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Decision boundary for a $15$-NN classifier (source: ESL)\relax }}{74}{figure.caption.63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Computational implementation of $k$-nearest neighbors}{74}{subsection.10.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Test error of a $k$-NN classifier over $k$ (source: ESL)\relax }}{75}{figure.caption.64}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces General shape of a bias-variance tradeoff (source: ESL)\relax }}{75}{figure.caption.65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Other sample spaces}{76}{subsection.10.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Summary}{76}{subsection.10.6}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Classification Trees}{76}{section.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Tree-induced, axis-parallel partitions of $\mathbb  {R}^d$}{77}{subsection.11.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}The Regression Tree hypothesis class}{77}{subsection.11.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A tree and its induced partition in $\ensuremath  {\mathbb  {R}}^2$\relax }}{78}{figure.caption.66}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces A partition into axis-aligned boxes that is {\bf  not} a tree partition. (Source: ESL)\relax }}{78}{figure.caption.67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Any hypothesis in $\mathcal  {H}_{CT}$ corresponds to a Decision Tree, and vice-versa}{78}{subsection.11.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}How {\em  not} to grow a classification tree}{79}{subsection.11.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5}How to grow a classification tree}{81}{subsection.11.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6}Growing classification Trees a-la CART}{81}{subsection.11.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7}Why to prune a classification tree}{82}{subsection.11.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8}How to make predictions on a new sample}{83}{subsection.11.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9}Interpretability}{83}{subsection.11.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10}Summary- Classification Trees}{83}{subsection.11.10}}
\newlabel{part:lecture_4}{{11.10}{84}{Lecture 4: PAC Theory of Statistical Learning, Part I}{part*.68}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_4}}{84}{part*.68}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Introduction}{84}{section.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Immunotherapy Data Set\relax }}{84}{figure.caption.69}}
\@writefile{toc}{\contentsline {section}{\numberline {13}A Theoretical framework for learning}{85}{section.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}A Data-generation Model }{85}{subsection.13.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Classifiers}{85}{subsection.13.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}The framework so far}{86}{subsection.13.3}}
\newlabel{sec:frame}{{13.3}{86}{The framework so far}{subsection.13.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Our goal in this lecture}{86}{subsection.13.4}}
\@writefile{toc}{\contentsline {paragraph}{Definition: PAC learnability and sample complexity}{86}{section*.70}}
\@writefile{toc}{\contentsline {paragraph}{Definition: VC-Dimension}{86}{section*.71}}
\@writefile{toc}{\contentsline {paragraph}{The Fundamental Theorem of Statistical Learning.}{87}{section*.72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5}Learning as a Game - first attempt}{87}{subsection.13.5}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Probably correct \& Approximately correct learners}{88}{section.14}}
\@writefile{toc}{\contentsline {paragraph}{Second question:}{89}{section*.73}}
\@writefile{toc}{\contentsline {paragraph}{Definition:}{90}{section*.74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}The game - for Probably Approximately correct learners}{91}{subsection.14.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Connection between $m$,$\delta $, and $\epsilon $.\relax }}{92}{figure.caption.75}}
\@writefile{toc}{\contentsline {section}{\numberline {15}No Free Lunch and Hypothesis Classes}{92}{section.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}No Free Lunch!}{92}{subsection.15.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}We need hypothesis classes}{93}{subsection.15.2}}
\@writefile{toc}{\contentsline {paragraph}{Realizability Assumption.}{93}{section*.76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Updating the game one last time}{94}{subsection.15.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Example: Threshold functions}{95}{subsection.15.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Possible training data for $\mathcal  {H}_{th}$\relax }}{96}{figure.caption.79}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Possible training data for $\mathcal  {H}_{th}$\relax }}{96}{figure.caption.80}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Possible training data for $\mathcal  {H}_{th}$\relax }}{96}{figure.caption.81}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Forbidden training data for $\mathcal  {H}_{th}$ - violates the Realizability Assumption\relax }}{96}{figure.caption.82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.1}Threshold functions - conclusion}{97}{subsubsection.15.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {16}PAC learning}{97}{section.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Finite hypothesis classes are PAC learnable}{98}{subsection.16.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.1.1}Empirical Risk Minimization}{98}{subsubsection.16.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.1.2}Learning Finite Classes}{99}{subsubsection.16.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {17}VC Dimension}{101}{section.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1}Motivation}{101}{subsection.17.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2}Formal Definition}{102}{subsection.17.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3}Exercises to help you understand the definition of VC-dimension}{102}{subsection.17.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {17.3.1}Axis aligned rectangles}{102}{subsubsection.17.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {17.3.2}Finite classes}{103}{subsubsection.17.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {17.3.3}Half-spaces through the origin}{103}{subsubsection.17.3.3}}
\newlabel{part:lecture_5}{{17.3.3}{104}{Lecture 5: PAC Theory of Statistical Learning, Part II}{part*.97}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_5}}{104}{part*.97}}
\@writefile{toc}{\contentsline {section}{\numberline {18}Recap of PAC Theory so far}{104}{section.18}}
\@writefile{toc}{\contentsline {paragraph}{Framework.}{104}{section*.99}}
\@writefile{toc}{\contentsline {paragraph}{The Learning Game.}{104}{section*.100}}
\@writefile{toc}{\contentsline {section}{\numberline {19}Finite Hypothesis Classes are PAC learnable}{108}{section.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.0.1}Empirical Risk Minimization}{108}{subsubsection.19.0.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.0.2}Learning Finite Classes}{109}{subsubsection.19.0.2}}
\@writefile{toc}{\contentsline {section}{\numberline {20}The Fundamental Theorem of Statistical Learning}{111}{section.20}}
\@writefile{toc}{\contentsline {section}{\numberline {21}Agnostic PAC: Extending our theoretical framework}{112}{section.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1}Moving from a probability distribution over $\mathcal  {X}$ to a joint probability distribution over $\mathcal  {X}\times \mathcal  {Y}$}{113}{subsection.21.1}}
\@writefile{toc}{\contentsline {paragraph}{Exercise.}{113}{section*.114}}
\newlabel{L:def}{{2}{114}{Exercise}{equation.21.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2}Removing the realizability assumption}{114}{subsection.21.2}}
\@writefile{toc}{\contentsline {paragraph}{Definition: Bayes optimal predictor}{114}{section*.115}}
\@writefile{toc}{\contentsline {paragraph}{Definition:}{115}{section*.116}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.3}Introducing a general loss function}{115}{subsection.21.3}}
\@writefile{toc}{\contentsline {paragraph}{Definition: general loss function.}{115}{section*.117}}
\@writefile{toc}{\contentsline {paragraph}{Definition: generalization loss induced by a general loss function.}{115}{section*.118}}
\@writefile{toc}{\contentsline {paragraph}{Definition:}{116}{section*.119}}
\@writefile{toc}{\contentsline {section}{\numberline {22}Agnostic-PAC learnability}{116}{section.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1}Probably Approximately correct learner - in the new framework.}{116}{subsection.22.1}}
\@writefile{toc}{\contentsline {paragraph}{Exercise.}{116}{section*.120}}
\@writefile{toc}{\contentsline {paragraph}{Exercise.}{116}{section*.121}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2}Agnostic-PAC learnability}{116}{subsection.22.2}}
\newlabel{apac:def}{{21}{116}{Agnostic-PAC learnability}{definition.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Exercise.}{116}{section*.122}}
\@writefile{toc}{\contentsline {paragraph}{The Learning Game.}{116}{section*.123}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3}PAC learnability is equivalent to Agnostic-PAC learnability}{117}{subsection.22.3}}
\@writefile{toc}{\contentsline {paragraph}{Theorem.}{117}{section*.124}}
\@writefile{toc}{\contentsline {section}{\numberline {23}Back to the Fundamental Theorem of Statistical Learning}{117}{section.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1}Empirical Risk Minimization strikes again}{117}{subsection.23.1}}
\@writefile{toc}{\contentsline {paragraph}{Definition: Empirical Risk}{117}{section*.125}}
\@writefile{toc}{\contentsline {paragraph}{Definition: ERM learner in the Agnostic-PAC framework.}{118}{section*.126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.2}ERM makes sense due to WLLN}{118}{subsection.23.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.3}The Fundamental Theorem - now with Agnostic-PAC}{118}{subsection.23.3}}
\@writefile{toc}{\contentsline {section}{\numberline {24}A Taste of the Proof}{119}{section.24}}
\@writefile{toc}{\contentsline {paragraph}{Part Two of the Fundamental Theorem: The ERM learner is a universal learner for any $\mathcal  {H}$ with finite VC-dimension.}{119}{section*.127}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.1}The uniform convergence property}{119}{subsection.24.1}}
\@writefile{toc}{\contentsline {paragraph}{Definition.}{120}{section*.128}}
\newlabel{approx:lem}{{2}{120}{Definition}{lemma.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Exercise:}{120}{section*.129}}
\@writefile{toc}{\contentsline {paragraph}{Definition.}{120}{section*.130}}
\@writefile{toc}{\contentsline {paragraph}{Exercise.}{120}{section*.131}}
\@writefile{toc}{\contentsline {section}{\numberline {25}Proving that if $VCdim(\mathcal  {H})<\infty $ then $\mathcal  {H}$ has the uniform convergence property}{120}{section.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {25.1}Achieving uniformity in both $\mathcal  {H}$ and $\mathcal  {D}$}{121}{subsection.25.1}}
\newlabel{F:eq}{{4}{121}{Achieving uniformity in both $\Hc $ and $\Dc $}{equation.25.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {25.2}The case of finite $\mathcal  {H}$}{121}{subsection.25.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {25.3}The general case - infinite $\mathcal  {H}$}{122}{subsection.25.3}}
\@writefile{toc}{\contentsline {paragraph}{Definition.}{122}{section*.132}}
\@writefile{toc}{\contentsline {paragraph}{Definition.}{123}{section*.133}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {25.3.1}First part of the proof: if \bf  $|\mathcal  {H}_C|$ grows polynomially in $|C|$ then $\mathcal  {H}$ has the uniform convergence property}{123}{subsubsection.25.3.1}}
\newlabel{wish:eq}{{5}{123}{First part of the proof: if \bf $|\Hc _C|$ grows polynomially in $|C|$ then $\Hc $ has the uniform convergence property}{equation.25.5}{}}
\newlabel{F:lem}{{3}{123}{First part of the proof: if \bf $|\Hc _C|$ grows polynomially in $|C|$ then $\Hc $ has the uniform convergence property}{lemma.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {25.3.2}If $VCdim(\mathcal  {H})<\infty $, $|\mathcal  {H}_C|$ only grows polynomially in $|C|$}{124}{subsubsection.25.3.2}}
\newlabel{perles:lem}{{4}{124}{If $VCdim(\Hc )<\infty $, $|\Hc _C|$ only grows polynomially in $|C|$}{lemma.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {25.3.3}Summary}{124}{subsubsection.25.3.3}}
\newlabel{part:lecture_6}{{25.3.3}{125}{Lecture 6: Ensemble Methods - Bagging and Boosting}{part*.134}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_6}}{125}{part*.134}}
\@writefile{toc}{\contentsline {section}{\numberline {26}Introduction}{125}{section.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.1}Bias/Variance}{126}{subsection.26.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces the bias-variance tradeoff\relax }}{127}{figure.caption.136}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.2}Ensemble / Committee methods}{127}{subsection.26.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.3}The uncorrelated case.}{127}{subsection.26.3}}
\@writefile{toc}{\contentsline {paragraph}{Accuracy of the committee's decision.}{127}{section*.137}}
\@writefile{toc}{\contentsline {paragraph}{Variance of the committee's decision.}{128}{section*.138}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.4}The correlated case.}{128}{subsection.26.4}}
\@writefile{toc}{\contentsline {paragraph}{Accuracy of the committee's decision.}{128}{section*.139}}
\@writefile{toc}{\contentsline {paragraph}{Variance of the committee's decision.}{128}{section*.140}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Simulation: wisdom of the crowd. The probability of the committee being right (and its standard deviation in errorbars) overlay with the probability (and standard deviation) of each member individually being right (ESL figure 8.11)\relax }}{129}{figure.caption.141}}
\newlabel{fig:crowds}{{30}{129}{Simulation: wisdom of the crowd. The probability of the committee being right (and its standard deviation in errorbars) overlay with the probability (and standard deviation) of each member individually being right (ESL figure 8.11)\relax }{figure.caption.141}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.5}Summary}{129}{subsection.26.5}}
\@writefile{toc}{\contentsline {section}{\numberline {27}Committee methods in machine learning}{129}{section.27}}
\@writefile{toc}{\contentsline {paragraph}{Committee methods - Definition.}{130}{section*.142}}
\@writefile{toc}{\contentsline {section}{\numberline {28}The Bootstrap}{130}{section.28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {28.1}Why does the Bootstrap work?}{130}{subsection.28.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces CDF of a probability distribution on the real line, and empirical CDF of an i.i.d sample from that distribution. Black lines on the horizontal axis show the random sample. \relax }}{131}{figure.caption.143}}
\newlabel{ecdf}{{31}{131}{CDF of a probability distribution on the real line, and empirical CDF of an i.i.d sample from that distribution. Black lines on the horizontal axis show the random sample. \relax }{figure.caption.143}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces The Great Baron Munchausen pulling himself up\relax }}{132}{figure.caption.144}}
\@writefile{toc}{\contentsline {section}{\numberline {29}Bagging}{132}{section.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Collection of Bagged Decision Trees. (Source: ESL)\relax }}{133}{figure.caption.145}}
\@writefile{toc}{\contentsline {paragraph}{Handling repeated samples.}{133}{section*.146}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.1}This is shockingly effective}{133}{subsection.29.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Improvement of Bagging Decision Trees over a single tree. From the original paper \emph  { Shang and Breiman, Distribution Based Trees Are More Accurate}\relax }}{134}{figure.caption.147}}
\newlabel{breiman}{{34}{134}{Improvement of Bagging Decision Trees over a single tree. From the original paper \emph { Shang and Breiman, Distribution Based Trees Are More Accurate}\relax }{figure.caption.147}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Test error of simple Bagging of decision trees, over $T$ the number of bagged trees (Source: ESL)\relax }}{134}{figure.caption.148}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.2}Bagging reduces variance}{134}{subsection.29.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.3}Decorrelation}{135}{subsection.29.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.4}Random Forest: Bagging of Decision Trees + De-correlation}{135}{subsection.29.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Test error of simple Bagging of decision trees (no de-correlation), Random Forests, and Gradient Boosting of Trees. (Source: ESL)\relax }}{136}{figure.caption.149}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.5}Some discussion points about Bagging}{136}{subsection.29.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Left: Decision boundary of a Random Forest classifier. Right: Decision boundary of a $3-NN$ classifier. Observe that the Random Forest tends to have axis-parallel boundaries. (Source: ESL)\relax }}{137}{figure.caption.155}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.6}Random Forest classifier summary}{137}{subsection.29.6}}
\@writefile{toc}{\contentsline {section}{\numberline {30}Boosting}{138}{section.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Original problem. Uniform distribution $D^1$.\relax }}{139}{figure.caption.156}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Iteration $1$. Left: $h_1$ with $D^1$. Right: $D^2$ \relax }}{139}{figure.caption.157}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Iteration $2$. Left: $h_1$ with $D^1$. Center: $h_2$ with $D^2$. Right: $D^3$\relax }}{139}{figure.caption.158}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Iterations $3$. Left: $h_1$ with $D^1$. Center: $h_2$ with $D^2$. Right: $h^3$ with $D^3$\relax }}{140}{figure.caption.159}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces The Boosting committee\relax }}{140}{figure.caption.160}}
\@writefile{toc}{\contentsline {subsection}{\numberline {30.1}Classification problem with a weighted sample}{140}{subsection.30.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {30.2}Adaboost}{141}{subsection.30.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {30.3}PAC view of boosting}{143}{subsection.30.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {30.4}Bias and variance in boosting}{144}{subsection.30.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {30.5}It's often better to Boost very simple learners}{144}{subsection.30.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Test error of boosting decision stumps (single level decision trees), with Adaboost over the number of boosting iterations $T$.\relax }}{145}{figure.caption.161}}
\@writefile{toc}{\contentsline {section}{\numberline {31}Bagging vs Boosting - Comparison}{145}{section.31}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Bagging and Boosting\relax }}{145}{table.caption.162}}
\@writefile{toc}{\contentsline {section}{\numberline {32}Summary}{146}{section.32}}
\newlabel{part:lecture_7}{{32}{147}{Lecture 7: Regression, Regularization, Model Selection and Model Evaluation}{part*.163}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_7}}{147}{part*.163}}
\@writefile{toc}{\contentsline {section}{\numberline {33}Introduction}{147}{section.33}}
\@writefile{toc}{\contentsline {section}{\numberline {34}Regularization}{147}{section.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {34.1}The setup: Choosing $h\in \mathcal  {H}$ by minimizing fidelity}{147}{subsection.34.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {34.2}Adding a regularization term}{148}{subsection.34.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {34.3}Let's focus on Euclidean sample space, regression problems and ERM fidelity for the square loss}{149}{subsection.34.3}}
\@writefile{toc}{\contentsline {section}{\numberline {35}CART Regression Trees}{150}{section.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {35.1}Regression Trees}{150}{subsection.35.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {35.2}Growing a CART regression tree}{151}{subsection.35.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {35.3}Pruning a CART regression tree - using regularization}{152}{subsection.35.3}}
\newlabel{T}{{6}{153}{Pruning a CART regression tree - using regularization}{equation.35.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {35.4}The complete Random Forest algorithms for regression and for classification}{153}{subsection.35.4}}
\@writefile{toc}{\contentsline {section}{\numberline {36}Modern Regression methods on $\ensuremath  {\mathbb  {R}}^d$}{153}{section.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.1}Linear Regression with high-dimensional data}{153}{subsection.36.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.2}Best subset selection}{154}{subsection.36.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.3}$\ell _0$ regularization: Best subset selection}{156}{subsection.36.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.4}Ridge}{156}{subsection.36.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Left to right: Original weights $\mathcal  {V}{w}$ in a linear model $\mathcal  {V}{y}=X^\ensuremath  {\top }\mathcal  {V}{w} + noise$, weights fitted by linear regression ($\lambda =0$), weights fitted by Ridge ($\lambda >0$). Source: Ryan Tibshirani Data Mining slides, CMU 36-462/36-662 \relax }}{156}{figure.caption.165}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Bias, variance and MSE (mean square error) of a regression problem using Ridge regression, over $\lambda $. Source: Ryan Tibshirani Data Mining slides, CMU 36-462/36-662 \relax }}{157}{figure.caption.166}}
\@writefile{toc}{\contentsline {paragraph}{Regularization Path.}{158}{section*.167}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Regularization Path of a Ridge regression, over $\lambda $. Source: Ryan Tibshirani Data Mining slides, CMU 36-462/36-662 \relax }}{158}{figure.caption.168}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.5}Lasso}{158}{subsection.36.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces A Lasso\relax }}{159}{figure.caption.169}}
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces Right: Ridge weights. Left: Lasso weights on the same problem. Note how in Lasso, many coefficients are sent to $0$, so that the weight vector is {\bf  sparse.} Source: Ryan Tibshirani Data Mining slides, CMU 36-462/36-662 \relax }}{159}{figure.caption.170}}
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces Right: Ridge regularization path. Left: Lasso regularization path on the same problem. Note how in Lasso, many coefficients are sent to $0$, so that the weight vector is {\bf  sparse.} Source: Ryan Tibshirani Data Mining slides, CMU 36-462/36-662 \relax }}{160}{figure.caption.171}}
\@writefile{toc}{\contentsline {paragraph}{Computational considerations.}{160}{section*.172}}
\@writefile{toc}{\contentsline {paragraph}{The Lasso is a kind of subset selection.}{160}{section*.173}}
\@writefile{toc}{\contentsline {section}{\numberline {37}The $\ell _1$ norm induces sparsity}{160}{section.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {37.1}Unit balls}{160}{subsection.37.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces The constrained optimization problems in $d=2$. Left: Lasso ($\ell _1$ ball). Right: Ridge ($\ell _2$ ball). Source: ESL\relax }}{161}{figure.caption.174}}
\@writefile{toc}{\contentsline {subsection}{\numberline {37.2}Orthogonal design}{162}{subsection.37.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces The constrained optimization problems in $d=2$. Left: Lasso ($\ell _1$ ball). Right: Ridge ($\ell _2$ ball). Source: ESL\relax }}{163}{figure.caption.175}}
\@writefile{toc}{\contentsline {section}{\numberline {38}$\ell _1$-regularized logistic regression}{163}{section.38}}
\@writefile{toc}{\contentsline {section}{\numberline {39}Practical considerations}{164}{section.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.1}Choosing lambda}{164}{subsection.39.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.2}Intercept}{164}{subsection.39.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.3}Software implementation}{164}{subsection.39.3}}
\@writefile{toc}{\contentsline {section}{\numberline {40}Introduction}{164}{section.40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {40.1}Model Selection}{164}{subsection.40.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {40.2}Model Evaluation}{165}{subsection.40.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {40.3}What is the generalization error, exactly?}{165}{subsection.40.3}}
\@writefile{toc}{\contentsline {section}{\numberline {41}Bias-Variance}{166}{section.41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {41.1}Bias-Variance decomposition for square error loss}{166}{subsection.41.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces Bias-Variance decomposition in polynomial fitting. Horizontal axis: fitted polynomial degree.\relax }}{167}{figure.caption.177}}
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Bias-Variance decomposition in simulation. Left: $k$-NN regression. Right: Best-subset regression. Horizontal axis - left: $k$ the number of nearest neighbors, right: $p$ the subset size. Orange - generalization error, green - square bias, blue - variance. Source: ESL\relax }}{168}{figure.caption.178}}
\@writefile{toc}{\contentsline {subsection}{\numberline {41.2}The bias-variance tradeoff}{168}{subsection.41.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces The general phenomenon of Bias-Variance tradeoff over model complexity. Source: ESL\relax }}{169}{figure.caption.179}}
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Bias-Variance tradeoff in simulation. Left: $k$-NN classification. Right: Classification using Best-subset regression. Horizontal axis - left: $k$ the number of nearest neighbors, right: $p$ the subset size. Orange - generalization error, green - square bias, blue - variance. Source: ESL\relax }}{169}{figure.caption.180}}
\@writefile{toc}{\contentsline {section}{\numberline {42}Can't naively use training sample for model selection / evaluation}{170}{section.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {56}{\ignorespaces Train error and generalization error over model complexity. Blue is training error (empirical risk), Red is generalization error. Each thin line is a different random sample of training sample (blue) and test sample (red). Source: ESL\relax }}{171}{figure.caption.181}}
\@writefile{toc}{\contentsline {section}{\numberline {43}Model selection and evaluation with unlimited data}{171}{section.43}}
\@writefile{toc}{\contentsline {section}{\numberline {44}$k$-fold Cross Validation}{172}{section.44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {44.1}Cross validation for model selection}{172}{subsection.44.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {57}{\ignorespaces Plot produced by the package {\tt  glmnet} for choosing $\lambda $ for regularized regression by cross validation\relax }}{173}{figure.caption.182}}
\@writefile{toc}{\contentsline {subsection}{\numberline {44.2}Cross validation for model evaluation}{173}{subsection.44.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {58}{\ignorespaces  Generalization error (orange) and 10-fold cross-validation error (with errorbars, blue) over model complexity for some learning problem. Source: ESL\relax }}{174}{figure.caption.183}}
\@writefile{toc}{\contentsline {subsection}{\numberline {44.3}Considerations in choosing number of folds $k$}{174}{subsection.44.3}}
\@writefile{toc}{\contentsline {section}{\numberline {45}Bootstrap}{174}{section.45}}
\@writefile{toc}{\contentsline {section}{\numberline {46}Two common mistakes in model evaluation and how to avoid them}{175}{section.46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.1}Over-estimating generalization error}{175}{subsection.46.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {59}{\ignorespaces {\bf  Exercise}: Suppose this is the generalization error of a learner $\mathcal  {A}$ as function of training sample size. We would like to estimate generalization error of $\mathcal  {A}$ using $k$-fold cross validation. Suppose our data has $m=200$ samples. Will 5-fold CV give a good estimator of out-of-sample error? Now suppose $m=50$. Has anything changed? Source: ESL \relax }}{175}{figure.caption.184}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.2}Under-estimating generalization error}{176}{subsection.46.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.3}What can we do to avoid under-estimating generalization error?}{176}{subsection.46.3}}
\newlabel{part:lecture_8}{{46.3}{178}{Lecture 8: Unsupervised Learning}{part*.185}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_8}}{178}{part*.185}}
\@writefile{toc}{\contentsline {section}{\numberline {47}Unsupervised learning: Introduction}{178}{section.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {60}{\ignorespaces Head tracking data. (Source: visagetechnologies.com)\relax }}{178}{figure.caption.187}}
\@writefile{lof}{\contentsline {figure}{\numberline {61}{\ignorespaces Handwritten digits. (Source: Liu et al, Handwritten digit recognition, Pattern Recognition 37(2) 2004)\relax }}{179}{figure.caption.188}}
\@writefile{lof}{\contentsline {figure}{\numberline {62}{\ignorespaces Digit 3 in a two-dimensional space. Green dots correspond to images of handwritten ``3'' in the dataset. Red circles correspond to images shown. (Source: ESL)\relax }}{179}{figure.caption.189}}
\@writefile{lof}{\contentsline {figure}{\numberline {63}{\ignorespaces Faces of four people from the Yale face dataset\relax }}{180}{figure.caption.190}}
\@writefile{lof}{\contentsline {figure}{\numberline {64}{\ignorespaces Faces above - dimension reduced to $2$, and clustered. Each of the four people is marked with a different marker. (Source: UML)\relax }}{181}{figure.caption.191}}
\@writefile{toc}{\contentsline {subsection}{\numberline {47.1}This lecture}{181}{subsection.47.1}}
\@writefile{toc}{\contentsline {section}{\numberline {48}Dimension reduction}{181}{section.48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.1}Linear dimension reduction}{182}{subsection.48.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {65}{\ignorespaces Data close to a linear subspace of $R^d$\relax }}{182}{figure.caption.192}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.2}Principal Components Analysis (PCA)}{183}{subsection.48.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.3}PCA as Variance Maximization}{184}{subsection.48.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.4}PCA - formal definition.}{185}{subsection.48.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.5}Applying PCA dimension reduction to an arbitrary vector in $\ensuremath  {\mathbb  {R}}^d$}{185}{subsection.48.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.6}The subtle difference between the projected points and their coordinates.}{186}{subsection.48.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {66}{\ignorespaces Left: points $\mathcal  {V}{x}_1,\ldots  ,\mathcal  {V}{x}_m$ are projected on the subspace spanned by the two leading eigenvectors of $A$. Right: coordinates of the projected points according to these two orthonormal vectors. (Source: ESL)\relax }}{186}{figure.caption.193}}
\@writefile{lof}{\contentsline {figure}{\numberline {67}{\ignorespaces (Source: CMU 36-462/36-662)\relax }}{187}{figure.caption.194}}
\@writefile{lof}{\contentsline {figure}{\numberline {68}{\ignorespaces (Source: CMU 36-462/36-662)\relax }}{187}{figure.caption.195}}
\@writefile{lof}{\contentsline {figure}{\numberline {69}{\ignorespaces Faces of four people from the Yale face dataset\relax }}{188}{figure.caption.196}}
\@writefile{lof}{\contentsline {figure}{\numberline {70}{\ignorespaces Faces above projected to $10$ dimensional subspace found by PCA. Source: UML\relax }}{188}{figure.caption.197}}
\@writefile{lof}{\contentsline {figure}{\numberline {71}{\ignorespaces Faces above - dimension reduced to $2$. Each of the four people is marked with a different marker. (Source: UML)\relax }}{189}{figure.caption.198}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.7}Interpreting and using the principal vectors as ``typical data points''}{189}{subsection.48.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {72}{\ignorespaces Source: ESL\relax }}{189}{figure.caption.199}}
\@writefile{lof}{\contentsline {figure}{\numberline {73}{\ignorespaces Source: Hao et al, Facial Recognition Using Tensor-Tensor Decompositions, SIAM Journal on Imaging Sciences 2013\relax }}{190}{figure.caption.200}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.8}Practical considerations.}{190}{subsection.48.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {48.8.1}Fast computation of PCA}{190}{subsubsection.48.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {48.8.2}Choosing $k$}{191}{subsubsection.48.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {74}{\ignorespaces Typical plot of principal components over their index. (Source: Wikipedia)\relax }}{191}{figure.caption.201}}
\@writefile{toc}{\contentsline {section}{\numberline {49}Clustering}{192}{section.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {75}{\ignorespaces (Source: Shai Shalev-Shwartz slides)\relax }}{192}{figure.caption.202}}
\@writefile{lof}{\contentsline {figure}{\numberline {76}{\ignorespaces (Source: Shai Shalev-Shwartz slides)\relax }}{193}{figure.caption.203}}
\@writefile{lof}{\contentsline {figure}{\numberline {77}{\ignorespaces (Source: Shai Shalev-Shwartz slides)\relax }}{193}{figure.caption.204}}
\@writefile{toc}{\contentsline {subsection}{\numberline {49.1}k-means}{194}{subsection.49.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {78}{\ignorespaces (Source: CMU 36-462/36-662)\relax }}{195}{figure.caption.205}}
\@writefile{lof}{\contentsline {figure}{\numberline {79}{\ignorespaces (Source: CMU 36-462/36-662)\relax }}{196}{figure.caption.206}}
\@writefile{lof}{\contentsline {figure}{\numberline {80}{\ignorespaces Source: ESL\relax }}{196}{figure.caption.207}}
\newlabel{part:lecture_9}{{49.1}{197}{Lecture 9: ML Actually}{part*.208}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_9}}{197}{part*.208}}
\newlabel{part:lecture_10}{{49.1}{198}{Lecture 10: Convex Optimization and SGD}{part*.209}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_10}}{198}{part*.209}}
\newlabel{part:lecture_11}{{49.1}{199}{Lecture 11: Online Learning and RL}{part*.210}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_11}}{199}{part*.210}}
\newlabel{part:lecture_12}{{49.1}{200}{Lecture 12: Deep Learning}{part*.211}{}}
\@writefile{toc}{\contentsline {part}{\nameref  {part:lecture_12}}{200}{part*.211}}
